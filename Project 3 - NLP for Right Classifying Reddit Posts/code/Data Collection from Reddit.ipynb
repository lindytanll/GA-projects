{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1eedfc5c",
   "metadata": {},
   "source": [
    "# Project 3: NLP for Right-Classifying Reddit Posts (Data Collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025e737",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287886a1",
   "metadata": {},
   "source": [
    "- With increasing number of working moms, and dads having to play an increasing role in taking care of children, child rearing becomes increasing a shared responsibility.\n",
    "\n",
    "- As an employee of theAsianparent, we help parents experience healthy pregnancies and raise healthy families. Through this project, we hope to:\n",
    "    - Automate the classification and monitoring of posts and discussions by moms and dads\n",
    "    - Better understand concerns and topics of interest amongst them to create targeted and interesting articles to better support parenthood\n",
    "\n",
    "- Target Audience: Moms and dads\n",
    "\n",
    "- As want to curate topics and articles for both moms and dads, we will use accuracy, sensitivity (proportion of /mommit posts being right classified) and specificity (proportion of /daddit posts being right classified) to measure the best performing model.\n",
    "\n",
    "- We will be exploring using Logistic Regression, Multinomial Naive Bayes and Random Forest with count vectorizer and TF-IDF vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59ddb64",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a6bed8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c7d5829",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_posts(total_posts, topic):\n",
    "\n",
    "    n_posts = 0\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    url = 'https://api.pushshift.io/reddit/search/submission'\n",
    "    params = {'subreddit': topic, 'size': 100} \n",
    "\n",
    "    while n_posts < total_posts:\n",
    "        res = requests.get(url, params)\n",
    "        data = res.json()\n",
    "        posts = data['data']\n",
    "        date = posts[len(posts)-1]['created_utc']\n",
    "        params['before'] = date\n",
    "        # len(posts)\n",
    "        df_new = pd.DataFrame(posts)\n",
    "        df_new = df_new[['subreddit', 'selftext', 'title']]\n",
    "        df_new = df_new[(df_new['selftext'] != \"\") & (df_new['selftext'] != \"[removed]\") & (df_new['selftext'] != \"[deleted]\") & (df_new['selftext'].notnull())]\n",
    "        df = pd.concat([df, df_new], ignore_index=True)\n",
    "        df.drop_duplicates(subset=['selftext'], keep='last', inplace=True)\n",
    "        n_posts = df.shape[0]\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d683e166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# web-scrap 10,000 posts from r/mommit and save to mommit.csv\n",
    "mommit = get_reddit_posts(10000, 'Mommit').to_csv('..\\data\\mommit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efab46a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web-scrap 10,000 posts from r/daddit and save to daddit.csv\n",
    "daddit = get_reddit_posts(10000, 'Daddit').to_csv('..\\data\\daddit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
